{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based reinforcement learning\n",
    "\n",
    "- Leer transitiefunctie $T(s, a, s')$ van de omgeving\n",
    "- Ontdek reward functie $R(s, a, s')$ wanneer we (state-actie-state) tupels ervaren\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'A': 0.24999987500018747,\n",
       "  'B': 2.499996250005625e-07,\n",
       "  'C': 2.499996250005625e-07,\n",
       "  'D': 0.7499991250013125,\n",
       "  'E': 2.499996250005625e-07,\n",
       "  'x': 2.499996250005625e-07},\n",
       " {'A': -1, 'B': -inf, 'C': -inf, 'D': -1, 'E': -inf, 'x': -inf})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [\n",
    "    # episode 1\n",
    "    ['B', 'east', 'C', -1],\n",
    "    ['C', 'east', 'D', -1],\n",
    "    ['D', 'exit', 'x', 10],\n",
    "    # episode 2\n",
    "    ['B', 'east', 'C', -1],\n",
    "    ['C', 'east', 'D', -1],\n",
    "    ['D', 'exit', 'x', 10],\n",
    "    # episode 3\n",
    "    ['E', 'north','C', -1],\n",
    "    ['C', 'east', 'D', -1],\n",
    "    ['D', 'exit', 'x', 10],\n",
    "    # episode 4\n",
    "    ['E', 'north','C', -1],\n",
    "    ['C', 'east', 'A', -1],\n",
    "    ['A', 'exit', 'x', -10],\n",
    "]\n",
    "\n",
    "states = ['A', 'B', 'C', 'D', 'E', 'x']\n",
    "actions = ['east', 'west', 'south', 'north', 'exit']\n",
    "\n",
    "# 3-dimensionale dictionary (zeer inefficient)\n",
    "T = {s : {a : {sn : 1e-6 for sn in states} for a in actions} for s in states}\n",
    "R = {s : {a : {sn : float('-inf') for sn in states} for a in actions} for s in states}\n",
    "\n",
    "# evaluate each state-action-state triplet \n",
    "for s, a, sn, r in dataset:\n",
    "    R[s][a][sn] = r\n",
    "    T[s][a][sn] += 1\n",
    "\n",
    "# normalize transitions to sum to 1 for each (s, a)\n",
    "for s in states:\n",
    "    for a in actions:\n",
    "        summed_rewards = sum(T[s][a].values())\n",
    "        T[s][a] = {key: val / summed_rewards for key, val in T[s][a].items()}\n",
    "\n",
    "T['C']['east'], R['C']['east']"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-free reinforcement learning\n",
    "\n",
    "- Leer een directe representatie van waarde van states/actions\n",
    "- **Temporal difference learning**:\n",
    "  - Leer de waarderingsfunctie $V(s)$ (in het boek $U(s)$): hoe goed is het voor de agent om in state $s$ te zijn?\n",
    "  - Leer de kwaliteitsfunctie $Q(s, a)$: hoe goed is het voor de agent om in state $s$ actie $a$ uit te voeren?\n",
    "\n",
    "#### Waarderingsfunctie $V(s)$\n",
    "\n",
    "- *Temporal difference* omdat we kijken naar het verschil tussen \n",
    "  - de huidige waarde van een state, $V(s)$ \n",
    "  - en de discounted waarde van de volgende state plus reward, $\\gamma V(s') + R(s, a, s')$\n",
    "  - waarbij $\\gamma$ de discount factor voorstelt\n",
    "- Update de huidige waarde $V(s)$ met dit (temporaal) verschil gewogen door een learning rate $\\alpha$\n",
    "- Dus $V(s) \\leftarrow V(s) + \\alpha [\\gamma V(s') + R(s, a, s') - V(s)]$ \n",
    "- Dit kunnen we ook opschrijven als:\n",
    "$$\n",
    "    V(s) \\leftarrow \\underbrace{(1-\\alpha)V(s)}_{\\text{aandeel oud}} + \\underbrace{\\alpha (R(s, a, s') + \\gamma V(s'))}_{\\text{aandeel nieuw}}\n",
    "$$\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 5.0, 'B': -1.0, 'C': 1.5625, 'D': 8.75, 'E': 1.75, 'x': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [\n",
    "    # episode 1\n",
    "    ['B', 'east', 'C', -1],\n",
    "    ['C', 'east', 'D', -1],\n",
    "    ['D', 'exit', 'x', 10],\n",
    "    # episode 2\n",
    "    ['B', 'east', 'C', -1],\n",
    "    ['C', 'east', 'D', -1],\n",
    "    ['D', 'exit', 'x', 10],\n",
    "    # episode 3\n",
    "    ['E', 'north','C', -1],\n",
    "    ['C', 'east', 'D', -1],\n",
    "    ['D', 'exit', 'x', 10],\n",
    "    # episode 4\n",
    "    ['E', 'north','C', -1],\n",
    "    ['C', 'east', 'A', -1],\n",
    "    ['A', 'exit', 'x', 10],\n",
    "]\n",
    "\n",
    "V = {s: 0 for s in states}\n",
    "\n",
    "alpha = 0.5 # learning rate\n",
    "gamma = 1   # discount\n",
    "\n",
    "for s, a, sn, r in dataset:\n",
    "    # V[s] = (1-alpha) * V[s] + alpha * (r + gamma * V[sn])\n",
    "    V[s] += alpha * (r + gamma * V[sn] - V[s])\n",
    "\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Welke actie gekozen wordt is nu impliciet (passive learning agent).\n",
    "- In plaats van $V(s)$ kunnen we het alternatief $Q(s, a)$ gebruiken (active learning agent), waarmee we ook acties $a$ kunnen selecteren. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kwaliteitsfunctie $Q(s, a)$\n",
    "\n",
    "- $V(s)$ zegt hoe goed het is voor de agent om in state $s$ te zijn\n",
    "- $Q(s, a)$ zegt hoe goed het is om in state $s$ actie $a$ uit te voeren\n",
    "- De relatie tussen $V(s)$ en $Q(s, a)$ is:\n",
    "$$\n",
    "    V(s) = \\max_a Q(s, a)\n",
    "$$\n",
    "- Triviaal: vervang $V(s)$ in de update rule om de Q-learning update rule te krijgen:\n",
    "\\begin{align*}\n",
    "    Q(s, a) &\\leftarrow Q(s, a) + \\alpha [\\gamma \\max_{a'} Q(s', a') + R(s, a, s') - Q(s, a)]\\\\\n",
    "    Q(s, a) &\\leftarrow \\underbrace{(1-\\alpha)Q(s, a)}_{\\text{aandeel oud}} + \\underbrace{\\alpha (R(s, a, s') + \\gamma \\max_{a'} Q(s', a'))}_{\\text{aandeel nieuw}}\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'east': 0, 'west': 0, 'south': 0, 'north': 0, 'exit': 5.0},\n",
       " 'B': {'east': -0.75, 'west': 0, 'south': 0, 'north': 0, 'exit': 0},\n",
       " 'C': {'east': 1.5625, 'west': 0, 'south': 0, 'north': 0, 'exit': 0},\n",
       " 'D': {'east': 0, 'west': 0, 'south': 0, 'north': 0, 'exit': 8.75},\n",
       " 'E': {'east': 0, 'west': 0, 'south': 0, 'north': 1.75, 'exit': 0},\n",
       " 'x': {'east': 0, 'west': 0, 'south': 0, 'north': 0, 'exit': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [\n",
    "    # episode 1\n",
    "    ['B', 'east', 'C', -1],\n",
    "    ['C', 'east', 'D', -1],\n",
    "    ['D', 'exit', 'x', 10],\n",
    "    # episode 2\n",
    "    ['B', 'east', 'C', -1],\n",
    "    ['C', 'east', 'D', -1],\n",
    "    ['D', 'exit', 'x', 10],\n",
    "    # episode 3\n",
    "    ['E', 'north','C', -1],\n",
    "    ['C', 'east', 'D', -1],\n",
    "    ['D', 'exit', 'x', 10],\n",
    "    # episode 4\n",
    "    ['E', 'north','C', -1],\n",
    "    ['C', 'east', 'A', -1],\n",
    "    ['A', 'exit', 'x', 10],\n",
    "]\n",
    "\n",
    "states = ['A', 'B', 'C', 'D', 'E', 'x']\n",
    "actions = ['east', 'west', 'south', 'north', 'exit']\n",
    "\n",
    "Q = {s : {a : 0 for a in actions} for s in states}\n",
    "alpha = 0.5\n",
    "gamma = 1\n",
    "\n",
    "for s, a, sn, r in dataset:\n",
    "    max_Q_sn = max(Q[sn].values())\n",
    "    Q[s][a] = (1-alpha) * Q[s][a] + alpha * (r + gamma * max_Q_sn)\n",
    "\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('east', 1.5625)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(Q['C'].items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numpy als dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  0,  2, -1],\n",
       "        [ 2,  0,  3, -1],\n",
       "        [ 3,  4,  5, 10],\n",
       "        [ 1,  0,  2, -1],\n",
       "        [ 2,  0,  3, -1],\n",
       "        [ 3,  4,  5, 10],\n",
       "        [ 4,  3,  2, -1],\n",
       "        [ 2,  0,  3, -1],\n",
       "        [ 3,  4,  5, 10],\n",
       "        [ 4,  3,  2, -1],\n",
       "        [ 2,  0,  0, -1],\n",
       "        [ 0,  4,  5, 10]]),\n",
       " array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "stoi = {s:i for i, s in enumerate(states)}  # state to int\n",
    "atoi = {a:i for i, a in enumerate(actions)} # action to int\n",
    "\n",
    "num_dataset = [[stoi[s], atoi[a], stoi[sn], r] for s, a, sn, r in dataset]\n",
    "num_dataset = np.array(num_dataset)\n",
    "\n",
    "Q_num = np.zeros((len(states), len(actions)))\n",
    "\n",
    "num_dataset, Q_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    ,  0.    ,  0.    ,  0.    ,  5.    ],\n",
       "       [-0.75  ,  0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [ 1.5625,  0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  8.75  ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  1.75  ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s, a, sn, r in num_dataset:\n",
    "    Q_num[s, a] = (1-alpha) * Q_num[s, a] + alpha * (r + gamma * Q_num[sn].max())\n",
    "\n",
    "Q_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5625"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_num[stoi['C'], atoi['east']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'east'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kiezen van beste volgende actie\n",
    "best_idx = Q_num[stoi['C']].argmax()\n",
    "actions[best_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs Exploitation\n",
    "\n",
    "- Hoe komen we aan de dataset van $(s, a, s', r)$ tupels?\n",
    "- enerzijds at random verkennen van de omgeving (exploration)\n",
    "- anderzijds beste actie kiezen (exploitation)\n",
    "- We kunnen $\\epsilon$-greedy search gebruiken, waarbij\n",
    "    - we uniform een random actie kiezen met kans $\\epsilon$\n",
    "    - we de beste actie kiezen met kans $1-\\epsilon$\n",
    "- Belangrijke afweging: welke waarde kies je voor $\\epsilon$ (hyperparameter)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace_jan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
